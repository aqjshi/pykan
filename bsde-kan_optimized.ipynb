{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from kan import KAN, LBFGS\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import autograd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "params\n",
    "'''\n",
    "S = 2\n",
    "T = 1\n",
    "t = 0\n",
    "tau = T-t\n",
    "K = 5\n",
    "sigma = 0.13\n",
    "r = 0.03\n",
    "x = np.log(S)\n",
    "X_max = 2*K\n",
    "\n",
    "dim = 2\n",
    "np_i = 100  # Number of interior points (along each dimension)\n",
    "np_b = 100  # Number of boundary points (along each dimension)\n",
    "ranges_X = [0, X_max]\n",
    "ranges_tau = [0, T]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "FUNCTION DEFINITIONS\n",
    "'''\n",
    "def black_scholes(S, K, T, r, sigma):\n",
    "    d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * tau) / (sigma * np.sqrt(tau))\n",
    "    d2 = d1 - sigma * np.sqrt(T)\n",
    "    price = S * norm.cdf(d1) - K * np.exp(-r * tau) * norm.cdf(d2)\n",
    "    return price\n",
    "\n",
    "def batch_jacobian(func, x, create_graph=False):\n",
    "    # x in shape (Batch, Length)\n",
    "    def _func_sum(x):\n",
    "        return func(x).sum(dim=0)\n",
    "    \n",
    "    return autograd.functional.jacobian(_func_sum, x, create_graph=create_graph).permute(1, 0, 2)\n",
    "\n",
    "def helper(X, Y):\n",
    "    return torch.stack([X.reshape(-1,), Y.reshape(-1,)]).permute(1, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "solu\n",
    "'''\n",
    "\n",
    "# Define the solution functions\n",
    "d1 = lambda x: (x[:, [0]] - np.log(K) + (r + 0.5 * sigma**2) * x[:, [1]]) / (sigma * np.sqrt(x[:, [1]]))\n",
    "d2 = lambda x: d1(x)-sigma*np.sqrt((x[:, [1]]))\n",
    "\n",
    "sol_fun = lambda x: np.exp(x[:, [0]]) * norm.cdf(d1(x)) - K * np.exp(-r * x[:, [1]]) * norm.cdf(d2(x))\n",
    "source_fun = lambda x: 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "data prep\n",
    "'''\n",
    "# Define grid and sampling\n",
    "device = \"cpu\"\n",
    "sampling_mode = 'random'  # Options: 'random' or 'mesh'\n",
    "x_mesh = torch.linspace(ranges_X[0], ranges_X[1], steps=np_i)\n",
    "y_mesh = torch.linspace(ranges_tau[0], ranges_tau[1], steps=np_i)\n",
    "X, Y = torch.meshgrid(x_mesh, y_mesh, indexing=\"ij\")\n",
    "\n",
    "if sampling_mode == 'mesh':\n",
    "    x_i = helper(X, Y)\n",
    "else:\n",
    "    x_i = torch.rand((np_i**2, 1), device=device) * (ranges_X[1] - ranges_X[0]) + ranges_X[0]\n",
    "    y_i = torch.rand((np_i**2, 1), device=device) * (ranges_tau[1] - ranges_tau[0]) + ranges_tau[0]\n",
    "    x_i = torch.cat([x_i, y_i], dim=1)\n",
    "\n",
    "# Split into 80% train and 20% test\n",
    "num_train = int(0.8 * x_i.shape[0])\n",
    "indices = torch.randperm(x_i.shape[0])\n",
    "\n",
    "train_indices = indices[:num_train]\n",
    "test_indices = indices[num_train:]\n",
    "\n",
    "x_i_train = x_i[train_indices]\n",
    "x_i_test = x_i[test_indices]\n",
    "\n",
    "# Prepare boundary data\n",
    "xb1 = helper(X[0], Y[1])\n",
    "xb2 = helper(X[-1], Y[0])\n",
    "xb3 = helper(X[:, 0], Y[:, 1])\n",
    "x_b = torch.cat([xb1, xb2, xb3], dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "model def\n",
    "'''\n",
    "\n",
    "# Define the model\n",
    "model = KAN(width=[2, 2, 1], grid=5, k=3, grid_eps=1.0, noise_scale=0.25)\n",
    "\n",
    "def train(steps=50, alpha=0.01, log=1):\n",
    "    optimizer = LBFGS(model.parameters(), lr=1, history_size=10, line_search_fn=\"strong_wolfe\", tolerance_grad=1e-32, tolerance_change=1e-32, tolerance_ys=1e-32)\n",
    "\n",
    "    pbar = tqdm(range(steps), desc='Training')  # Progress bar for training\n",
    "    for _ in pbar:\n",
    "        def closure():\n",
    "            global pde_loss, bc_loss\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Compute interior loss using training data\n",
    "            sol_train = sol_fun(x_i_train)\n",
    "            sol_D1_fun = lambda x: batch_jacobian(model, x, create_graph=True)[:, 0, :]\n",
    "            sol_D1 = sol_D1_fun(x_i_train)\n",
    "            sol_D2 = batch_jacobian(sol_D1_fun, x_i_train, create_graph=True)[:, :, :]\n",
    "            lap = torch.sum(torch.diagonal(sol_D2, dim1=1, dim2=2), dim=1, keepdim=True)\n",
    "            source = source_fun(x_i_train)\n",
    "            pde_loss = torch.mean((lap - source)**2)\n",
    "\n",
    "            # Compute boundary loss\n",
    "            bc_true = sol_fun(x_b)\n",
    "            bc_pred = model(x_b)\n",
    "            bc_loss = torch.mean((bc_pred - bc_true)**2)\n",
    "            \n",
    "            # Total loss is a weighted sum of PDE loss and boundary loss\n",
    "            loss = alpha * pde_loss + bc_loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            return loss\n",
    "\n",
    "        if _ % 5 == 0 and _ < 50:\n",
    "            model.update_grid_from_samples(x_i_train)  # Update grid from samples during training\n",
    "        \n",
    "        optimizer.step(closure)  # Perform a single optimization step\n",
    "\n",
    "        # Evaluate the model on the training data\n",
    "        sol_train = sol_fun(x_i_train)\n",
    "        l2_train = torch.mean((model(x_i_train) - sol_train)**2)\n",
    "\n",
    "        # Optionally, evaluate the model on the test data\n",
    "        sol_test = sol_fun(x_i_test)\n",
    "        l2_test = torch.mean((model(x_i_test) - sol_test)**2)\n",
    "\n",
    "        if _ % log == 0:\n",
    "            pbar.set_description(\"pde loss: %.2e | bc loss: %.2e | l2 train: %.2e | l2 test: %.2e\" % \n",
    "                                 (pde_loss.cpu().detach().numpy(), \n",
    "                                  bc_loss.cpu().detach().numpy(), \n",
    "                                  l2_train.detach().numpy(),\n",
    "                                  l2_test.detach().numpy()))\n",
    "\n",
    "train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pde loss: nan | bc loss: nan | l2 train: nan | l2 test: nan:  52%|█████▏    | 26/50 [00:22<00:21,  1.14it/s]                    \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# TRAINING AND EVALUATION\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Train the model with the updated train function including 80/20 train-test-validation split and 64/16 interior checking validation\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m train(model, x_i, x_b, sol_fun, source_fun, steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, log\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Plot the model's performance (e.g., prediction vs. true solution)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mplot(beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 45\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, x_i, x_b, sol_fun, source_fun, steps, alpha, log)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _ \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _ \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m50\u001b[39m:\n\u001b[1;32m     43\u001b[0m     model\u001b[38;5;241m.\u001b[39mupdate_grid_from_samples(x_i_train)  \u001b[38;5;66;03m# Update grid from samples during training\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep(closure)  \u001b[38;5;66;03m# Perform a single optimization step\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the training data\u001b[39;00m\n\u001b[1;32m     48\u001b[0m sol_train \u001b[38;5;241m=\u001b[39m sol_fun(x_i_train)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m             )\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/pykan-master/kan/LBFGS.py:443\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobj_func\u001b[39m(x, t, d):\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_directional_evaluate(closure, x, t, d)\n\u001b[0;32m--> 443\u001b[0m     loss, flat_grad, t, ls_func_evals \u001b[38;5;241m=\u001b[39m _strong_wolfe(\n\u001b[1;32m    444\u001b[0m         obj_func, x_init, t, d, loss, flat_grad, gtd)\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_grad(t, d)\n\u001b[1;32m    446\u001b[0m opt_cond \u001b[38;5;241m=\u001b[39m flat_grad\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m tolerance_grad\n",
      "File \u001b[0;32m~/Desktop/pykan-master/kan/LBFGS.py:100\u001b[0m, in \u001b[0;36m_strong_wolfe\u001b[0;34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001b[0m\n\u001b[1;32m     98\u001b[0m g_prev \u001b[38;5;241m=\u001b[39m g_new\u001b[38;5;241m.\u001b[39mclone(memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcontiguous_format)\n\u001b[1;32m     99\u001b[0m gtd_prev \u001b[38;5;241m=\u001b[39m gtd_new\n\u001b[0;32m--> 100\u001b[0m f_new, g_new \u001b[38;5;241m=\u001b[39m obj_func(x, t, d)\n\u001b[1;32m    101\u001b[0m ls_func_evals \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    102\u001b[0m gtd_new \u001b[38;5;241m=\u001b[39m g_new\u001b[38;5;241m.\u001b[39mdot(d)\n",
      "File \u001b[0;32m~/Desktop/pykan-master/kan/LBFGS.py:442\u001b[0m, in \u001b[0;36mLBFGS.step.<locals>.obj_func\u001b[0;34m(x, t, d)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobj_func\u001b[39m(x, t, d):\n\u001b[0;32m--> 442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_directional_evaluate(closure, x, t, d)\n",
      "File \u001b[0;32m~/Desktop/pykan-master/kan/LBFGS.py:291\u001b[0m, in \u001b[0;36mLBFGS._directional_evaluate\u001b[0;34m(self, closure, x, t, d)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_directional_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, closure, x, t, d):\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_grad(t, d)\n\u001b[0;32m--> 291\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(closure())\n\u001b[1;32m    292\u001b[0m     flat_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gather_flat_grad()\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_param(x)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[5], line 27\u001b[0m, in \u001b[0;36mtrain.<locals>.closure\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m sol_D1_fun \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: batch_jacobian(model, x, create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[:, \u001b[38;5;241m0\u001b[39m, :]\n\u001b[1;32m     26\u001b[0m sol_D1 \u001b[38;5;241m=\u001b[39m sol_D1_fun(x_i_train)\n\u001b[0;32m---> 27\u001b[0m sol_D2 \u001b[38;5;241m=\u001b[39m batch_jacobian(sol_D1_fun, x_i_train, create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[:, :, :]\n\u001b[1;32m     28\u001b[0m lap \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(torch\u001b[38;5;241m.\u001b[39mdiagonal(sol_D2, dim1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dim2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     29\u001b[0m source \u001b[38;5;241m=\u001b[39m source_fun(x_i_train)\n",
      "Cell \u001b[0;32mIn[2], line 16\u001b[0m, in \u001b[0;36mbatch_jacobian\u001b[0;34m(func, x, create_graph)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_func_sum\u001b[39m(x):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(x)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m autograd\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mjacobian(_func_sum, x, create_graph\u001b[38;5;241m=\u001b[39mcreate_graph)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/functional.py:675\u001b[0m, in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[1;32m    672\u001b[0m is_inputs_tuple, inputs \u001b[38;5;241m=\u001b[39m _as_tuple(inputs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacobian\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    673\u001b[0m inputs \u001b[38;5;241m=\u001b[39m _grad_preprocess(inputs, create_graph\u001b[38;5;241m=\u001b[39mcreate_graph, need_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 675\u001b[0m outputs \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m    676\u001b[0m is_outputs_tuple, outputs \u001b[38;5;241m=\u001b[39m _as_tuple(\n\u001b[1;32m    677\u001b[0m     outputs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs of the user-provided function\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacobian\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    678\u001b[0m )\n\u001b[1;32m    679\u001b[0m _check_requires_grad(outputs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, strict\u001b[38;5;241m=\u001b[39mstrict)\n",
      "Cell \u001b[0;32mIn[2], line 15\u001b[0m, in \u001b[0;36mbatch_jacobian.<locals>._func_sum\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_func_sum\u001b[39m(x):\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(x)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m, in \u001b[0;36mtrain.<locals>.closure.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Compute interior loss using training data\u001b[39;00m\n\u001b[1;32m     24\u001b[0m sol_train \u001b[38;5;241m=\u001b[39m sol_fun(x_i_train)\n\u001b[0;32m---> 25\u001b[0m sol_D1_fun \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: batch_jacobian(model, x, create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[:, \u001b[38;5;241m0\u001b[39m, :]\n\u001b[1;32m     26\u001b[0m sol_D1 \u001b[38;5;241m=\u001b[39m sol_D1_fun(x_i_train)\n\u001b[1;32m     27\u001b[0m sol_D2 \u001b[38;5;241m=\u001b[39m batch_jacobian(sol_D1_fun, x_i_train, create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[:, :, :]\n",
      "Cell \u001b[0;32mIn[2], line 16\u001b[0m, in \u001b[0;36mbatch_jacobian\u001b[0;34m(func, x, create_graph)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_func_sum\u001b[39m(x):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(x)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m autograd\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mjacobian(_func_sum, x, create_graph\u001b[38;5;241m=\u001b[39mcreate_graph)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/functional.py:787\u001b[0m, in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[1;32m    785\u001b[0m jac_i: Tuple[List[torch\u001b[38;5;241m.\u001b[39mTensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m([] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs)))  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(out\u001b[38;5;241m.\u001b[39mnelement()):\n\u001b[0;32m--> 787\u001b[0m     vj \u001b[38;5;241m=\u001b[39m _autograd_grad(\n\u001b[1;32m    788\u001b[0m         (out\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[j],),\n\u001b[1;32m    789\u001b[0m         inputs,\n\u001b[1;32m    790\u001b[0m         retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    791\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    792\u001b[0m     )\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m el_idx, (jac_i_el, vj_el, inp_el) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[1;32m    795\u001b[0m         \u001b[38;5;28mzip\u001b[39m(jac_i, vj, inputs)\n\u001b[1;32m    796\u001b[0m     ):\n\u001b[1;32m    797\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m vj_el \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/functional.py:193\u001b[0m, in \u001b[0;36m_autograd_grad\u001b[0;34m(outputs, inputs, grad_outputs, create_graph, retain_graph, is_grads_batched)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m,) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs)\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(\n\u001b[1;32m    194\u001b[0m         new_outputs,\n\u001b[1;32m    195\u001b[0m         inputs,\n\u001b[1;32m    196\u001b[0m         new_grad_outputs,\n\u001b[1;32m    197\u001b[0m         allow_unused\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    198\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    199\u001b[0m         retain_graph\u001b[38;5;241m=\u001b[39mretain_graph,\n\u001b[1;32m    200\u001b[0m         is_grads_batched\u001b[38;5;241m=\u001b[39mis_grads_batched,\n\u001b[1;32m    201\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:436\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    432\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    433\u001b[0m         grad_outputs_\n\u001b[1;32m    434\u001b[0m     )\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 436\u001b[0m     result \u001b[38;5;241m=\u001b[39m _engine_run_backward(\n\u001b[1;32m    437\u001b[0m         t_outputs,\n\u001b[1;32m    438\u001b[0m         grad_outputs_,\n\u001b[1;32m    439\u001b[0m         retain_graph,\n\u001b[1;32m    440\u001b[0m         create_graph,\n\u001b[1;32m    441\u001b[0m         inputs,\n\u001b[1;32m    442\u001b[0m         allow_unused,\n\u001b[1;32m    443\u001b[0m         accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    444\u001b[0m     )\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    447\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    449\u001b[0m     ):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    769\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    770\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "eval \n",
    "'''\n",
    "# Evaluation and plotting\n",
    "model.plot(beta=10)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KAN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
